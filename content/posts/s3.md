---
title: "S3"
date: 2020-07-24T20:08:36+07:00
draft: false
---

# Amazon Simple Storage Service

Buckets are containers for objects. You can have one or more buckets. You can control access for each bucket, deciding who can create, delete, and list objects in it. You can also choose the geographical Region where Amazon S3 will store the bucket and its contents and view access logs for the bucket and its objects.

We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks.

You are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket.

The AWS Solutions site lists many of the ways you can use Amazon S3. The following list summarizes some of those ways.

* Backup and storage – Provide data backup and storage services for others.
* Application hosting – Provide services that deploy, install, and manage web applications.
* Media hosting – Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads.
* Software delivery – Host your software applications that customers can download.

Making requests using federated user temporary credentials

You can request temporary security credentials and provide them to your federated users or applications who need to access your AWS resources. 

Both the AWS account and an IAM user can request temporary security credentials for federated users. However, for added security, only an IAM user with the necessary permissions should request these temporary credentials to ensure that the federated user gets at most the permissions of the requesting IAM user. In some applications, you might find it suitable to create an IAM user with specific permissions for the sole purpose of granting temporary security credentials to your federated users and applications.

# Bucket restrictions and limitations

A bucket is owned by the AWS account that created it. Bucket ownership is not transferable.
When you create a bucket, you choose its name and the Region to create it in. After you create a bucket, you can't change its name or Region.
By default, you can create up to 100 buckets in each of your AWS accounts. If you need additional buckets, you can increase your account bucket limit to a maximum of 1,000 buckets by submitting a service limit increase. There is no difference in performance whether you use many buckets or just a few. 

# Reusing bucket names

If a bucket is empty, you can delete it. After a bucket is deleted, the name becomes available for reuse. However, after you delete the bucket, you might not be able to reuse the name for various reasons. For example, when you delete the bucket and the name becomes available for reuse, another account might create a bucket with that name. Additionally, some time might pass before you can reuse the name of a deleted bucket. If you want to use the same bucket name, we recommend that you don't delete the bucket.

# Objects and buckets

Bucket names must be unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (Standard Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud [US] Regions).
For best compatibility, we recommend that you avoid using dots (.) in bucket names, except for buckets that are used only for static website hosting. If you include dots in a bucket's name, you can't use virtual-host-style addressing over HTTPS, unless you perform your own certificate validation. This is because the security certificates used for virtual hosting of buckets don't work for buckets with dots in their names.

# Amazon S3 Transfer Acceleration

Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.

# Why Use Amazon S3 Transfer Acceleration?

You might want to use Transfer Acceleration on a bucket for various reasons, including the following:

You have customers that upload to a centralized bucket from all over the world.
You transfer gigabytes to terabytes of data on a regular basis across continents.
You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.
Amazon S3 Transfer Acceleration does not support cross region copies using PUT Object - Copy.
You can continue to use the regular endpoint in addition to the accelerate endpoints.
You can point your Amazon S3 PUT object and GET object requests to the s3-accelerate endpoint domain name after you enable Transfer Acceleration.
To accelerate the PUT you simply change the host name in your request to mybucket.s3-accelerate.amazonaws.com. 
To using the standard upload speed, simply change the name back to mybucket.s3.us-east-1.amazonaws.com.
After Transfer Acceleration is enabled, it can take up to 20 minutes for you to realize the performance benefit. However, the accelerate endpoint will be available as soon as you enable Transfer Acceleration.

# Requester Pays buckets

A bucket owner, however, can configure a bucket to be a Requester Pays bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.
If you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed.

Requester Pays buckets do not support the following.

* Anonymous requests
* BitTorrent
* SOAP requests

# Charge Details

bucket owner is charged for the request under the following conditions:

* The requester doesn't include the parameter x-amz-request-payer in the header (GET, HEAD, or POST) or as a parameter (REST) in the request (HTTP code 403).
* Request authentication fails (HTTP code 403).
* The request is anonymous (HTTP code 403).
* The request is a SOAP request.

# Using cost allocation S3 bucket tags

The cost allocation report lists the AWS usage for your account by product category and AWS Identity and Access Management (IAM) user. The report contains the same line items as the detailed billing report (see Understanding your AWS billing and usage reports for Amazon S3) and additional columns for your tag keys.

# AWS provides two types of cost allocation tags,

* AWS-generated tag 
* User-defined tags. 
You must activate both types of tags separately in the Billing and Cost Management console before they can appear in your billing reports.
Each S3 bucket has a tag set. A tag set contains all of the tags that are assigned to that bucket. 
Within a bucket, if you add a tag that has the same key as an existing tag, the new value overwrites the old value.

# Managing data access with Amazon S3 access points

Amazon S3 Access Points simplify managing data access at scale for shared datasets in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject. Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.

# Amazon S3 access points have the following restrictions and limitations:

* You can only use access points to perform operations on objects. You can't use access points to perform other Amazon S3 operations, such as modifying or deleting buckets}
* Access points work with some, but not all, AWS services and features. For example, you can't configure Cross-Region Replication to operate through an access point. 
* By default, you can create up to 1,000 access points per Region for each of your AWS accounts. If you need more than 1,000 access points for a single account in a single Region, you can request a service quota increase.
* You can create S3 access points using the AWS Management Console, AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API. 
* S3 access points aren't currently compatible with Amazon CloudWatch metrics.
* You can only create access points for buckets that you own.
* Each access point is associated with exactly one bucket, which you must specify when you create the access point. After you create an access point, you can't associate it with a different bucket. However, you can delete an access point and then create another one with the same name associated with a different bucket.
* After you create an access point, you can't change its virtual private cloud (VPC) configuration.
* Access point policies are limited to 20 KB in size.

# Amazon S3 storage classes

Each object in Amazon S3 has a storage class associated with it. 

## Storage classes for frequently accessed objects

For **performance-sensitive use cases (those that require millisecond access time)** and **frequently accessed data**, Amazon S3 provides the following storage classes:

* **S3 Standard** —The default storage class. If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class.

**Reduced Redundancy** —The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class.

## Storage class for automatically optimizing frequently and infrequently accessed objects

* **The S3 Intelligent**-Tiering storage class is designed to **optimize storage costs by automatically moving data to the most cost-effective storage access tier**, without performance impact or operational overhead.
* The Intelligent-Tiering storage class is **ideal if you want to optimize storage costs automatically for long-lived data when access patterns are unknown or unpredictable**.
* The S3 Intelligent-Tiering storage class stores objects in **two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequently accessed data**. 
* For a small monthly **monitoring and automation fee per object**, Amazon S3 monitors access patterns of the objects in the S3 Intelligent-Tiering storage class and **moves objects that have not been accessed for 30 consecutive days to the infrequent access tier**.
* There are **no retrieval fees** when using the S3 Intelligent-Tiering storage class. 
* The S3 Intelligent-Tiering storage class is suitable for **objects larger than 128 KB that you plan to store for at least 30 days**. If the size of an object is less than 128 KB, it is not eligible for auto-tiering. Smaller objects can be stored, but they are always charged at the frequent access tier rates in the S3 Intelligent-Tiering storage class.
* if you **delete an object before the end of the 30-day minimum storage duration period**, you are charged for 30 days. 


## Storage classes for infrequently accessed objects

The S3 Standard-IA and S3 One Zone-IA storage classes are designed for long-lived and infrequently accessed data. (IA stands for infrequent access.) S3 Standard-IA and S3 One Zone-IA objects are available for millisecond access (same as the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data.

**For example**, you might choose the S3 Standard-IA and S3 One Zone-IA storage classes:

For storing backups.

For older data that is accessed infrequently, but that still requires millisecond access. For example, when you upload data, you might choose the S3 Standard storage class, and use lifecycle configuration to tell Amazon S3 to transition the objects to the S3 Standard-IA or S3 One Zone-IA class.
S3 Standard-IA—Use for your primary or only copy of data that can't be re-created.

S3 One Zone-IA—Use if you can re-create the data if the Availability Zone fails, and for object replicas when setting S3 Cross-Region Replication (CRR).



## Storage classes for archiving objects

**S3 Glacier—Use for archives** where portions of the data might need to be **retrieved in minutes**. Data stored in the S3 Glacier storage class has a **minimum storage duration period of 90 days and can be accessed in as little as 1-5 minutes using expedited retrieval**. If you have deleted, overwritten, or transitioned to a different storage class an object before the 90-day minimum, you are charged for 90 days. For pricing information, see Amazon S3 pricing.

**S3 Glacier Deep Archive—Use for archiving data that rarely needs to be accessed**. Data stored in the S3 Glacier Deep Archive storage class has a **minimum storage duration period of 180 days** and a **default retrieval time of 12 hours**. If you have deleted, overwritten, or transitioned to a different storage class an object before the 180-day minimum, you are charged for 180 days. For pricing information, see Amazon S3 Pricing.

S3 Glacier Deep Archive is the **lowest cost storage option in AWS**. Storage costs for S3 Glacier Deep Archive are less expensive than using the S3 Glacier storage class. **You can reduce S3 Glacier Deep Archive retrieval costs by using bulk retrieval, which returns data within 48 hours**.

# Retrieving archived objects

You must first **restore the S3 Glacier and S3 Glacier Deep Archive objects before you can access them**. (S3 Standard, RRS, S3 Standard-IA, S3 One Zone-IA, and S3 Intelligent-Tiering objects are available for anytime access.) 


# Comparing the Amazon S3 storage classes

https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-compare

# Object subresources

**acl** - Contains a list of grants identifying the grantees and the permissions granted. When you create an object, the acl identifies the object owner as having full control over the object. You can retrieve an object ACL or replace it with an updated list of grants. Any update to an ACL requires you to replace the existing ACL.
**torrent** - Amazon S3 does not support the BitTorrent protocol in AWS Regions launched after May 30, 2016.


# Object Versioning

* S3 Versioning **protects you from the consequences of unintended overwrites and deletions**. You can also **use it to archive objects** so that you have **access to previous versions**.
* The SOAP API does not support S3 Versioning. SOAP support over HTTP is deprecated, but it is still available over HTTPS. New Amazon S3 features are not supported for SOAP.
* To customize your data retention approach and control storage costs, use object versioning with Object lifecycle management.
* If you have an object expiration lifecycle policy in your non-versioned bucket and you want to maintain the same permanent delete behavior when you enable versioning, you must add a **noncurrent expiration policy**. **The noncurrent expiration lifecycle policy will manage the deletes of the noncurrent object versions in the version-enabled bucket**. 
* You must explicitly enable S3 Versioning on your bucket. **By default, S3 Versioning is disabled**. Regardless of whether you have enabled Versioning, each object in your bucket has a version ID. 
* When you DELETE an object, all versions remain in the bucket and Amazon S3 inserts a delete marker
* The **delete marker** becomes the current version of the object. Performing a simple GET Object request when the current version is a delete marker returns a **404 Not Found error**
* You can GET a noncurrent version of an object by specifying its version ID.
* You can **permanently delete an object by specifying the version you want to delete**. Only the owner of an Amazon S3 bucket can permanently delete a version. 
* You can **add additional security by configuring a bucket to enable MFA Delete**.
* If you notice a **significant increase in the number of HTTP 503-slow down responses** received for **Amazon S3 PUT or DELETE object requests** to a bucket that has **S3 Versioning enabled**, you might have **one or more objects in the bucket for which there are millions of versions**.

# Object tagging

The benefits are as following:
* Object tags enable fine-grained access control of permissions. For example, you could grant an IAM user permissions to read-only objects with specific tags.
* Object tags enable fine-grained object lifecycle management in which you can specify a tag-based filter, in addition to a key name prefix, in a lifecycle rule.
* When using Amazon S3 analytics, you can configure filters to group objects together for analysis by object tags, by key name prefix, or by both prefix and tags.
* You can also customize Amazon CloudWatch metrics to display information by specific tag filters. The following sections provide details.
To add **object tag sets to more than one Amazon S3 object with a single request**, you can use **S3 Batch Operations**. You provide S3 Batch Operations with a list of objects to operate on. S3 Batch Operations call the respective API to perform the specified operation. A single S3 Batch Operations job can perform the specified operation on **billions of objects containing exabytes of data**.

S3 Batch Operations **track progress, send notifications, and store a detailed completion report of all actions, providing a fully managed, auditable, serverless experience**. You can use S3 Batch Operations through the AWS Management Console, AWS CLI, AWS SDKs, or REST API.

# Object lifecycle management

An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:

* Transition actions
* Expiration actions

**Transition actions** - Define when objects transition to another storage class. There are costs associated with the lifecycle transition requests. 
**Expiration actions** - Define when objects expire. Amazon S3 deletes expired objects on your behalf. The lifecycle expiration costs depend on when you choose to expire objects.

# When should I use lifecycle configuration?

S3 Lifecycle configuration rules, you can tell Amazon S3 to transition objects to less expensive storage classes, or archive or delete them.

# Transitioning objects 

[lifecycle-transitions](https://docs.aws.amazon.com/AmazonS3/latest/dev/images/lifecycle-transitions-v2.png)
Amazon S3 supports a waterfall model for transitioning between storage classes,

> Standard -> Standard-IA -> S3-IT-> s3 one zone IA -> glacier -> deep archive

The transition of objects to the S3 Glacier Deep Archive storage class can go only one way.

Archived objects are Amazon S3 objects, but before you can access an archived object, you must first restore a temporary copy of it. The restored object copy is available only for the duration you specify in the restore request. After that, Amazon S3 deletes the temporary copy, and the object remains archived in Amazon S3 Glacier.
The S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA storage classes have a minimum 30-day storage charge.
The minimal storage duration period is 90 days for the S3 Glacier storage class and 180 days for S3 Glacier Deep Archive. 
S3 does not transition objects that are smaller than 128 KB because it's not cost effective:

# Lifecycle and other bucket configurations

* You can add S3 Lifecycle configurations to unversioned buckets and versioning-enabled buckets.
* Lifecycle configuration on multi-factor authentication (MFA)-enabled buckets is not supported.
* Amazon S3 Lifecycle actions are not captured by AWS CloudTrail object level logging. CloudTrail captures API requests made to external Amazon S3 endpoints, whereas S3 Lifecycle actions are performed using internal Amazon S3 endpoints. 
* Amazon S3 server access logs can be enabled in an S3 bucket to capture S3 Lifecycle-related actions such as object transition to another storage class and object expiration resulting in permanent deletion or logical deletion.
* Amazon S3 server access log records are generally delivered on a best effort basis and cannot be used for complete accounting of all Amazon S3 requests.

# Storage Class Analysis

you can analyze storage access patterns to help you decide when to transition the right data to the right storage class.
you can use the analysis results to help you improve your lifecycle policies. You can configure storage class analysis to analyze all the objects in a bucket. Or, you can configure filters to group objects together for analysis by common prefix (that is, objects that have names that begin with a common string), by object tags, or by both prefix and tags.

# Managing Access with ACLs

Access control lists (ACLs) are one of the resource-based access policy options that you can use to manage access to your buckets and objects. 
You can use ACLs to grant basic read/write permissions to **other AWS accounts**. 
There are limits to managing permissions using ACLs. For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account. 
You cannot grant conditional permissions, nor can you explicitly deny permissions. ACLs are suitable for specific scenarios. For example, **if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using object ACL by the AWS account that owns the object**.

# Locking objects using S3 Object Lock

You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock helps you meet regulatory requirements that require WORM storage, or simply add another layer of protection against object changes and deletion.
Object Lock provides two ways to manage object retention: retention periods and legal holds.
A retention period specifies a fixed period of time during which an object remains locked. During this period, your object is WORM-protected and can't be overwritten or deleted.
A legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it. Legal holds are independent from retention periods.
Object Lock works only in versioned buckets, and retention periods and legal holds apply to individual object versions.
When you lock an object version, Amazon S3 stores the lock information in the metadata for that object version. Placing a retention period or legal hold on an object protects only the version specified in the request. It doesn't prevent new versions of the object from being created. If you put an object into a bucket that has the same key name as an existing, protected object, Amazon S3 creates a new version of that object, stores it in the bucket as requested, and reports the request as completed successfully. The existing, protected version of the object remains locked according to its retention configuration.

# Replication

Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can copy objects between different AWS Regions or within the same Region.

To enable object replication, you add a replication configuration to your source bucket. The minimum configuration must provide the following:

The destination bucket where you want Amazon S3 to replicate objects
An AWS Identity and Access Management (IAM) role that Amazon S3 can assume to replicate objects on your behalf

## Types of object replication

You can replicate objects between different AWS Regions or within the same AWS Region.

* **Cross-Region replication** - (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions.
* **Same-Region replication** - (SRR) is used to copy objects across Amazon S3 buckets in the same AWS Region.

## Why use replication

* Replicate objects while retaining metadata 
* Replicate objects into different storage classes
* Maintain object copies under different ownership 
* Replicate objects within 15 minutes

## When to use Same-Region replication(SRR)

* Aggregate logs into a single bucket 
* Configure live replication between production and test accounts 
* Abide by data sovereignty laws

## Requirements for replication

The source bucket owner must have the source and destination AWS Regions enabled for their account. The destination bucket owner must have the destination Region-enabled for their account. For more information about enabling or disabling an AWS Region, see AWS Service Endpoints in the AWS General Reference.
* Both source and destination buckets must have versioning enabled.
* Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket on your behalf.
* If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner READ and READ_ACP permissions with the object access control list (ACL). For more information, see Managing Access with ACLs.
* If the source bucket has S3 Object Lock enabled, the destination bucket must also have S3 Object Lock enabled. For more information, see Locking objects using S3 Object Lock.
* To enable replication on a bucket that has Object Lock enabled, contact AWS Support.
* If you are setting the replication configuration in a cross-account scenario, where source and destination buckets are owned by different AWS accounts, then owner of the destination bucket must grant the owner of the source bucket permissions to replicate objects with a bucket policy. For more information, see Granting permissions when source and destination buckets are owned by different AWS accounts.

## S3 Replication Time Control (S3 RTC)

S3 Replication Time Control (S3 RTC) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times. S3 RTC **replicates most objects that you upload to Amazon S3 in seconds, and 99.99 percent of those objects within 15 minutes**.

# S3 server access logging

* Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. 
* Server access logs don't record information about wrong-region redirect errors for Regions that launched after March 20, 2019. Wrong-region redirect errors occur when a request for an object or bucket is made outside the Region in which the bucket exists.
* In Amazon S3 you can grant permission to deliver access logs through bucket access control lists (ACLs), but not through bucket policy.
* Adding deny conditions to a bucket policy might prevent Amazon S3 from delivering access logs.
* Default bucket encryption on the target bucket can only be used if AES256 (SSE-S3) is selected. SSE-KMS encryption is not supported.
* S3 Object Lock cannot be enabled on the target bucket.
* source and target buckets must be in the **same AWS Region and owned by the same account**.

## Bucket logging status changes take effect over time

If you change the target bucket for logging from bucket A to bucket B, some logs for the next hour might continue to be delivered to bucket A, while others might be delivered to the new target bucket B. In all cases, the new settings eventually take effect without any further action on your part.
